{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a memory for the storage of structured semantic pointers\n",
    "\n",
    "In the previous examples, we have essentially ignored time by defining models that map inputs to outputs in a single forward pass (i.e., we configured the default synapse to be `None`). In this example, we'll introduce a simple process model of information retrieval based on [this](https://github.com/nengo/nengo/blob/master/examples/spa/question_memory.ipynb) standard Nengo tutorial. The idea is to sequentially present pairs of items that get bound together and added to a structured semantic pointer that is retained in memory over time. Once the bound pairs have been added to the semantic pointer, we can then query it with a cue as before to test retrieval accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo.spa as spa\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate random semantic pointers\n",
    "\n",
    "To start, we'll define a new function for generating training data that returns of arrays of shape `(n_inputs, n_steps, dims)`, where `n_steps` will be the number of time steps in the process we want to model. This data will allow us to learn model parameters that help to ensure a trajectory of inputs produces an appropriate trajectory of outputs. To start, we'll generate simple examples in which the input tractory presents a single semantic pointer to the network for some number of time steps, and the desired output trajectory involves maintaining a representation of this semantic pointer in a recurrently connected ensemble for some further number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_data(n_inputs, dims, seed, t_int, t_mem=None): \n",
    "    t_total = t_int + t_mem if t_mem else t_int * 2\n",
    "    \n",
    "    state = np.random.RandomState(seed)\n",
    "    vocab = spa.Vocabulary(dimensions=dims, rng=state, max_similarity=1)\n",
    "    \n",
    "    # intialize arrays for input and output trajectories\n",
    "    inputs = np.zeros((n_inputs, t_total, dims))\n",
    "    outputs = np.zeros((n_inputs, t_total, dims))\n",
    "    \n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_inputs):\n",
    "        vocab.add('SP' + str(n), vocab.create_pointer())\n",
    "        \n",
    "        # create inputs and target memory for first pair\n",
    "        inputs[n, :t_int, :] = vocab['SP' + str(n)].v\n",
    "        outputs[n, :, :] = vocab['SP' + str(n)].v\n",
    "\n",
    "    # make scaling ramp for target output trajectories\n",
    "    ramp = np.asarray([t / t_int for t in range(t_int)])\n",
    "    ramp = np.concatenate((ramp, np.ones(t_total - t_int)))\n",
    "    outputs = outputs * ramp[None, :, None]      \n",
    "        \n",
    "    return inputs, outputs, vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate some example trajectories for visualization purposes. We'll also make some arrays to define an input process in our model, so that an example input trajectory is presented by default when a simulation is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_int = 100\n",
    "n_inputs = 40\n",
    "dims = 64\n",
    "seed = 236\n",
    "default = 0 # which test item to present by default\n",
    "\n",
    "test_inputs, test_outputs, test_vocab = get_memory_data(n_inputs, dims, seed=seed, t_int=t_int)\n",
    "\n",
    "# create arrays for PresentInput process in model definition\n",
    "sp = np.vstack((test_vocab['SP' + str(default)].v, np.zeros((1, dims))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the model\n",
    "\n",
    "Initially, we'll build a very simple model containing a single input node and single memory ensemble. The input will by default present the arrays in `sp` at regular intervals specified by the variable `t_int`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 10 * dims  # number of neurons for memory ensemble\n",
    "p_time = t_int / 1000 # presentation interval for input process in ms\n",
    "\n",
    "with nengo.Network(seed=seed) as net:\n",
    "    nengo_dl.configure_settings(trainable=False)\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    \n",
    "    sp_input = nengo.Node(nengo.processes.PresentInput(sp, p_time))\n",
    "    memory = nengo.Ensemble(n_neurons, dims)\n",
    "\n",
    "    nengo.Connection(sp_input, memory)\n",
    "    conn = nengo.Connection(memory, memory, transform=0.5, synapse=0.1)\n",
    "    \n",
    "    net.config[conn].trainable = True\n",
    "    net.config[nengo.Probe].synapse = nengo.Lowpass(0.01)\n",
    "    \n",
    "    sp_probe = nengo.Probe(sp_input)   \n",
    "    memory_probe = nengo.Probe(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run the model for the specified number of steps, both to see how well the memory works, and to see how closely that trajectory of this memory's state matches the target trajectory. Because the initial transform on the memory's recurrent connection is less than one, we should expect the memory to be quite 'leaky' and not function particularly well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(2 * p_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_memory_example(sim, vocab):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    plot_vocab = vocab.create_subset([k for k in vocab.keys[:10]])\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[sp_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Input\")\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Memory\")\n",
    "\n",
    "    out = test_outputs[default,:,:].reshape(t_int*2, dims)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(out, plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Target Memory\")\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    \n",
    "plot_memory_example(sim, test_vocab)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "As is apparent in the above plot, the memory ensemble does not maintain a particularly stable representation of the input semantic pointer over time. This is because the transform weight on the memory's recurrent connection is 0.5, which results in the memory state decaying quickly over time. To improve rentention, we'll optimize the network parameters to correctly align the trajectories of the memory ensemble with the target trajectories corresponding to a large number of example input semantic pointers. Training on temporally extended trajectories can be slow, so we'll download pretrained parameters by default. You can train your own parameters under varying learning conditions by setting `do_training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=50, seed=seed)\n",
    "\n",
    "if do_training:\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.8, use_nesterov=True)\n",
    "    train_inputs, train_outputs, _ = get_memory_data(4000, dims, seed, t_int)\n",
    "    inputs = {sp_input: train_inputs}\n",
    "    outputs = {memory_probe: train_outputs}\n",
    "\n",
    "    print('Training loss before: ', sim.loss(inputs, outputs, 'mse'))\n",
    "    sim.train(input_feed, output_feed, optimizer, n_epochs=40, objective='mse')\n",
    "    print('Training loss after: ', sim.loss(inputs, outputs, 'mse'))\n",
    "\n",
    "    sim.save_params('./mem_params')\n",
    "\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4STdmenUxV2VPcTg\",\n",
    "        \"mem_params.zip\")\n",
    "    with zipfile.ZipFile(\"mem_params.zip\") as f:\n",
    "        f.extractall()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.load_params('./mem_params')\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "plot_memory_example(sim, test_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is apparent, the training procedure significantly improves the stability of the memory, with the representation of the input semantic pointer being retained indefinitely over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Introduce input binding and retrieval cues\n",
    "\n",
    "To make things a bit more complicated, we'll introduce a binding procedure such that the memory ensemble encodes a structured semantic pointer that is generated from a single pair of input semantic pointers. We'll also incorporate input cues that can be used to retrieve information from the memory ensemble. In order to do this, we'll have to make some simple changes to our data generation function and our model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binding_data(n_inputs, dims, seed, t_int, t_mem=None): \n",
    "    t_total = t_int + t_mem if t_mem else t_int * 2\n",
    "    \n",
    "    state = np.random.RandomState(seed)\n",
    "    vocab = spa.Vocabulary(dimensions=dims, rng=state, max_similarity=1)\n",
    "    \n",
    "    # intialize arrays for input and output trajectories\n",
    "    roles = np.zeros((n_inputs, t_total, dims))\n",
    "    fills = np.zeros((n_inputs, t_total, dims))\n",
    "    pairs = np.zeros((n_inputs, t_total, dims))\n",
    "\n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_inputs):        \n",
    "        roles[n, :t_int, :] = vocab.parse('ROLE_'+str(n)).v\n",
    "        fills[n, :t_int, :] = vocab.parse('FILL_'+str(n)).v\n",
    "        \n",
    "        pair_key = 'ROLE_' + str(n) + '*' + 'FILL_' + str(n)\n",
    "        pair_ptr = vocab.parse(pair_key)\n",
    "        vocab.add(pair_key, pair_ptr.v)\n",
    "        \n",
    "        pairs[n, :, :] = pair_ptr.v\n",
    "\n",
    "    # make scaling ramp for target output trajectories\n",
    "    ramp = np.asarray([t / t_int for t in range(t_int)])\n",
    "    ramp = np.concatenate((ramp, np.ones(t_total - t_int)))\n",
    "    pairs = pairs * ramp[None, :, None]\n",
    "\n",
    "    return roles, fills, pairs, vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing data\n",
    "test_roles, test_fills, test_pairs, test_vocab = get_binding_data(n_inputs, dims, seed=seed, t_int=t_int)\n",
    "\n",
    "# create arrays for PresentInput processes in model definition\n",
    "role = np.vstack((test_vocab['ROLE_'+str(default)].v, np.zeros((1, dims))))\n",
    "fill = np.vstack((test_vocab['FILL_'+str(default)].v, np.zeros((1, dims))))\n",
    "cue = np.vstack((np.zeros((1, dims)), test_vocab['ROLE_'+str(default)].v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this slightly more complicated model, we'll add a circular convolution network and train both the parameters of this network and the recurrent connection on the memory ensemble. We'll also add second circular convolution network for performing retrieval with an input cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    nengo_dl.configure_settings(trainable=False)\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    \n",
    "    role_inp = nengo.Node(nengo.processes.PresentInput(role, p_time))\n",
    "    fill_inp = nengo.Node(nengo.processes.PresentInput(fill, p_time))\n",
    "    cue_inp = nengo.Node(nengo.processes.PresentInput(cue, p_time))\n",
    "    \n",
    "    cconv = nengo.networks.CircularConvolution(50, dims)\n",
    "    ccorr = nengo.networks.CircularConvolution(50, dims, invert_b=True)\n",
    "    memory = nengo.Ensemble(n_neurons, dims)\n",
    "\n",
    "    nengo.Connection(role_inp, cconv.input_a)\n",
    "    nengo.Connection(fill_inp, cconv.input_b)\n",
    "    nengo.Connection(cconv.output, memory)\n",
    "    \n",
    "    nengo.Connection(memory, ccorr.input_a)\n",
    "    nengo.Connection(cue_inp, ccorr.input_b)\n",
    "    \n",
    "    mem_conn = nengo.Connection(memory, memory, transform=0.5, synapse=0.1)\n",
    "    \n",
    "    net.config[mem_conn].trainable = True\n",
    "    \n",
    "    net.config[nengo.Probe].synapse = nengo.Lowpass(0.01)\n",
    "    role_probe = nengo.Probe(role_inp) \n",
    "    fill_probe = nengo.Probe(fill_inp)\n",
    "    cue_probe = nengo.Probe(cue_inp)\n",
    "    conv_probe = nengo.Probe(cconv.output)\n",
    "    memory_probe = nengo.Probe(memory)\n",
    "    output_probe = nengo.Probe(ccorr.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test baseline retrieval accuracy\n",
    "\n",
    "We can use each role item in the collection of generated data as a cue, and each filler as a target for retrieval with this cue. Given as much, we can test retrieval accuracy using the same accuracy function that was used in the previous example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(sim, probe, vocab, targets, t_step=-1):\n",
    "    # provide a simulator instance, the probe being evaluated, the vocab,\n",
    "    # the target vectors, and the time step at which to evaluate\n",
    "\n",
    "    # get output at the given time step\n",
    "    output = sim.data[probe][:, t_step, :]\n",
    "\n",
    "    # compute similarity between each output and vocab item\n",
    "    sims = np.dot(vocab.vectors, output.T)\n",
    "    idxs = np.argmax(sims, axis=0)\n",
    "\n",
    "    # check that the output is most similar to the target\n",
    "    acc = np.mean(np.all(vocab.vectors[idxs] == targets[:, 0], axis=1))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # use fillers at midpoint of input process as targets\n",
    "targets = test_fills[:, int(t_int / 2), :][:, None, :]\n",
    "test_cues = np.flip(test_roles, 1) # use roles as cues\n",
    "\n",
    "# define an input feed for running the model on the test data\n",
    "test_inputs = {role_inp: test_roles, fill_inp: test_fills, cue_inp: test_cues}\n",
    "\n",
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=n_inputs) as sim:\n",
    "    sim.run(2 * p_time, input_feeds=test_inputs)\n",
    "\n",
    "print('Retrieval accuracy: ', accuracy(sim, output_probe, test_vocab, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that the model performs accurate retrieval only some of the time. Keep in mind too that the retrieval is assumed to be successful if an output vector is more similar to the correct filler vector than it is to any of the other vectors in the vocabulary, even if the overall similarity is quite low. We can visualize the model's output trajectories to explore this point in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "def plot_retrieval_example(sim, vocab):\n",
    "    plot_vocab = vocab.create_subset([k for k in vocab.keys[:8]])\n",
    "    plt.figure(figsize=(10, 14))\n",
    "    plt.subplot(7, 1, 1)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[role_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Role Input\")\n",
    "\n",
    "    plt.subplot(7, 1, 2)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[fill_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Filler Input\")\n",
    "\n",
    "    plt.subplot(7, 1, 3)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[conv_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Binding\")\n",
    "\n",
    "    plt.subplot(7, 1, 4)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Memory\")\n",
    "\n",
    "    out = test_pairs[default,:,:].reshape(t_int*2, dims)\n",
    "    plt.subplot(7, 1, 5)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(out, plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Target Memory\")\n",
    "\n",
    "    plt.subplot(7, 1, 6)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[cue_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Cue\")\n",
    "\n",
    "    plt.subplot(7, 1, 7)\n",
    "    plt.plot(sim.trange(), nengo.spa.similarity(sim.data[output_probe], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='x-small', loc='right')\n",
    "    plt.ylabel(\"Retrieval\")\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    \n",
    "plot_retrieval_example(sim, test_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the new model\n",
    "\n",
    "Since our memory ensemble is doing such a poor job of retaining the bound input items over time, it is unsurprising that the retrieval accuracy is mediocre. Notice too that even if the rerieved output vector is most similar to the correct target vector, it is still not very distinct from most of the other vectors in the vocabulary. We can fix this by optimizing the model parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=50, seed=seed)\n",
    "\n",
    "if do_training:\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.8, use_nesterov=True)\n",
    "\n",
    "    train_roles, train_fills, train_pairs, _ = get_binding_data(4000, dims, seed, t_int)\n",
    "    train_inputs = {role_inp: train_roles, fill_inp: train_fills}\n",
    "    train_outputs = {memory_probe: train_pairs}\n",
    "\n",
    "    print('Training loss before: ', sim.loss(train_inputs, train_outputs, 'mse'))\n",
    "    sim.train(input_feed, output_feed, optimizer, n_epochs=40, objective='mse')\n",
    "    print('Training loss after: ', sim.loss(train_inputs, train_outputs, 'mse'))\n",
    "    \n",
    "    sim.save_params('./mem_binding_params')\n",
    "\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4UlVnSTR2NEtMbkk\",\n",
    "        \"mem_binding_params.zip\")\n",
    "    with zipfile.ZipFile(\"mem_binding_params.zip\") as f:\n",
    "        f.extractall()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test improved retrieval accuracy\n",
    "\n",
    "Recomputing our accuracy measure on the test inputs indicates that our optimization procedure has been very successful - retrieval is now nearly flawless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=n_inputs) as sim:\n",
    "    sim.load_params('./mem_binding_params')\n",
    "    sim.run(2 * p_time, input_feeds=test_inputs)\n",
    "\n",
    "print('Retrieval accuracy: ', accuracy(sim, output_probe, test_vocab, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the change that results from training by plotting the similarities between the states of various network components and the vocabulary of known semantic pointers over time, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.load_params('./mem_binding_params')\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "plot_retrieval_example(sim, test_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
