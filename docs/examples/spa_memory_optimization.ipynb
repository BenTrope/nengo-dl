{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a memory for the storage of structured semantic pointers\n",
    "\n",
    "In the previous examples, we have essentially ignored time by defining models that map inputs to outputs in a single forward pass (i.e., we configured the default synapse to be `None`). In this example, we'll introduce a simple process model of information retrieval based on [this](https://github.com/nengo/nengo/blob/master/examples/spa/question_memory.ipynb) standard Nengo tutorial. The idea is to sequentially present pairs of items that get bound together and added to a structured semantic pointer that is retained in memory over time. Once the bound pairs have been added to the semantic pointer, we can then query it with a cue as before to test retrieval accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo.spa as spa\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate random semantic pointers\n",
    "\n",
    "To start, we'll define a new function for generating training data that returns of arrays of shape `(n_inputs, n_steps, dims)`, where `n_steps` will be the number of time steps in the process we want to model. This data will allow us to learn model parameters that help to ensure a trajectory of inputs produces an appropriate trajectory of outputs. To start, we'll generate simple examples in which the input tractory presents a single semantic pointer to the network for some number of time steps, and the desired output trajectory involves maintaining a representation of this semantic pointer in a recurrently connected ensemble for some further number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_data(n_inputs, dims, seed, t_int, t_mem=None): \n",
    "    t_total = t_int + t_mem if t_mem else t_int * 2\n",
    "    \n",
    "    state = np.random.RandomState(seed)\n",
    "    vocab = spa.Vocabulary(dimensions=dims, rng=state, max_similarity=1)\n",
    "    \n",
    "    # intialize arrays for input and output trajectories\n",
    "    inputs = np.zeros((n_inputs, t_total, dims))\n",
    "    outputs = np.zeros((n_inputs, t_total, dims))\n",
    "    \n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_inputs):\n",
    "        vocab.add('SP' + str(n), vocab.create_pointer())\n",
    "        \n",
    "        # create inputs and target memory for first pair\n",
    "        inputs[n, :t_int, :] = vocab['SP' + str(n)].v\n",
    "        outputs[n, :, :] = vocab['SP' + str(n)].v\n",
    "\n",
    "    # make scaling ramp for target output trajectories\n",
    "    ramp = np.asarray([t / t_int for t in range(t_int)])\n",
    "    ramp = np.concatenate((ramp, np.ones(t_total - t_int)))\n",
    "    outputs = outputs * ramp[None, :, None]      \n",
    "        \n",
    "    return inputs, outputs, vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate an example trajectory for visualization purposes. We'll also make some arrays to define an input process in our model, so that the example input trajectory is presented by default when a simulation is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_int = 100\n",
    "n_inputs = 10\n",
    "dims = 64\n",
    "seed = 236\n",
    "\n",
    "inputs, outputs, vocab = get_memory_data(n_inputs, dims, seed=seed, t_int=t_int)\n",
    "\n",
    "# create arrays for PresentInput process in model definition\n",
    "sp = np.vstack((vocab['SP0'].v, np.zeros((1, dims))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the model\n",
    "\n",
    "Initially, we'll build a very simple model containing a single input node and single memory ensemble. The input will by default present the arrays in `sp` at regular intervals specified by the variable `t_int`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 10 * dims  # number of neurons for memory ensemble\n",
    "p_time = t_int / 1000 # presentation interval for input process in ms\n",
    "\n",
    "with nengo.Network(seed=seed) as net:\n",
    "    nengo_dl.configure_settings(trainable=False)\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    \n",
    "    sp_input = nengo.Node(nengo.processes.PresentInput(sp, p_time))\n",
    "    memory = nengo.Ensemble(n_neurons, dims)\n",
    "\n",
    "    nengo.Connection(sp_input, memory)\n",
    "    conn = nengo.Connection(memory, memory, transform=0.5, synapse=0.1)\n",
    "    \n",
    "    net.config[conn].trainable = True\n",
    "    net.config[nengo.Probe].synapse = nengo.Lowpass(0.01)\n",
    "    \n",
    "    sp_probe = nengo.Probe(sp_input)   \n",
    "    memory_probe = nengo.Probe(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run the model for the specified number of steps, both to see how well the memory works, and to see how closely that trajectory of this memory's state matches the target trajectory. Because the initial transform on the memory's recurrent connection is less than one, we should expect the memory to be quite 'leaky' and not function particularly well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(2 * p_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[sp_probe], vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Input\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Memory\")\n",
    "\n",
    "out = outputs[0,:,:].reshape(t_int*2, dims)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(out, vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Target Memory\")\n",
    "plt.xlabel(\"time [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "As is apparent in the above plot, the memory ensemble does not maintain a particularly stable representation of the input semantic pointer over time. This is because the transform weight on the memory's recurrent connection is 0.5, which results in the memory state decaying quickly over time. To improv rentention, we'll optimize the network parameters to correctly align the trajectories of the memory ensemble with the target trajectories corresponding to a large number of example input semantic pointers. Training on temporally extended trajectories can be slow, so we'll download pretrained parameters by default. You can train your own parameters under varying learning conditions by setting `do_training=True` and then changing the code that defines the training data or the optimizer as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=50, seed=seed)\n",
    "\n",
    "if do_training:\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.8, use_nesterov=True)\n",
    "    train_inputs, train_outputs, _ = get_memory_data(4000, dims, seed, t_int)\n",
    "    input_feed = {sp_input: train_inputs}\n",
    "    output_feed = {memory_probe: train_outputs}\n",
    "\n",
    "    print('Training loss before: ', sim.loss(input_feed, output_feed, 'mse'))\n",
    "    sim.train(input_feed, output_feed, optimizer, n_epochs=40, objective='mse')\n",
    "    print('Training loss after: ', sim.loss(input_feed, output_feed, 'mse'))\n",
    "\n",
    "    sim.save_params('./mem_params')\n",
    "\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4STdmenUxV2VPcTg\",\n",
    "        \"mem_params.zip\")\n",
    "    with zipfile.ZipFile(\"mem_params.zip\") as f:\n",
    "        f.extractall()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.load_params('./mem_params')\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[sp_probe], vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Input\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Memory\")\n",
    "\n",
    "out = outputs[0,:,:].reshape(t_int*2, dims)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(out, vocab))\n",
    "plt.legend(vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Target Memory\")\n",
    "plt.xlabel(\"time [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is apparent, the training procedure significantly improves the stability of the memory, with the representation of the input semantic pointer being retained indefinitely over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Introduce input binding and retrieval cues\n",
    "\n",
    "To make things a bit more complicated, we'll introduce a binding procedure such that the memory ensemble encodes a structured semantic pointer that is generated from a pair of input semantic pointers. We'll also incorporate input cues that can be used to retrieve information from the memory ensemble. In order to do this, we'll have to update both our function for generating data, and our model definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binding_data(n_inputs, dims, seed, t_int, t_mem=None): \n",
    "    t_total = t_int + t_mem if t_mem else t_int * 2\n",
    "    \n",
    "    state = np.random.RandomState(seed)\n",
    "    vocab = spa.Vocabulary(dimensions=dims, rng=state, max_similarity=1)\n",
    "    \n",
    "    # intialize arrays for input and output trajectories\n",
    "    roles = np.zeros((n_inputs, t_total, dims))\n",
    "    fills = np.zeros((n_inputs, t_total, dims))\n",
    "    pairs = np.zeros((n_inputs, t_total, dims))\n",
    "\n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_inputs):        \n",
    "        roles[n, :t_int, :] = vocab.parse('ROLE'+str(n)).v\n",
    "        fills[n, :t_int, :] = vocab.parse('FILL'+str(n)).v\n",
    "        \n",
    "        pair_key = 'ROLE' + str(n) + '*' + 'FILL' + str(n)\n",
    "        pair_ptr = vocab.parse(pair_key)\n",
    "        vocab.add(pair_key, pair_ptr.v)\n",
    "        \n",
    "        pairs[n, :, :] = pair_ptr.v\n",
    "\n",
    "    # make scaling ramp for target output trajectories\n",
    "    ramp = np.asarray([t / t_int for t in range(t_int)])\n",
    "    ramp = np.concatenate((ramp, np.ones(t_total - t_int)))\n",
    "    pairs = pairs * ramp[None, :, None]\n",
    "\n",
    "    return roles, fills, pairs, vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_int = 100\n",
    "roles, fills, pairs, vocab = get_binding_data(n_inputs, dims, seed=seed, t_int=t_int)\n",
    "\n",
    "# create arrays for PresentInput processes in model definition\n",
    "role = np.vstack((vocab['ROLE0'].v, np.zeros((1, dims))))\n",
    "fill = np.vstack((vocab['FILL0'].v, np.zeros((1, dims))))\n",
    "cue = np.vstack((np.zeros((1, dims)), vocab['ROLE0'].v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this slightly more complicated model, we'll add a circular convolution network and train both the parameters of this network and the recurrent connection on the memory ensemble. We'll also add second circular convolution network for performing retrieval with an input cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    nengo_dl.configure_settings(trainable=False)\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    \n",
    "    role_inp = nengo.Node(nengo.processes.PresentInput(role, p_time))\n",
    "    fill_inp = nengo.Node(nengo.processes.PresentInput(fill, p_time))\n",
    "    cue_inp = nengo.Node(nengo.processes.PresentInput(cue, p_time))\n",
    "    \n",
    "    cconv = nengo.networks.CircularConvolution(50, dims)\n",
    "    ccorr = nengo.networks.CircularConvolution(50, dims, invert_b=True)\n",
    "    memory = nengo.Ensemble(n_neurons, dims)\n",
    "\n",
    "    nengo.Connection(role_inp, cconv.input_a)\n",
    "    nengo.Connection(fill_inp, cconv.input_b)\n",
    "    nengo.Connection(cconv.output, memory)\n",
    "    \n",
    "    nengo.Connection(memory, ccorr.input_a)\n",
    "    nengo.Connection(cue_inp, ccorr.input_b)\n",
    "    \n",
    "    mem_conn = nengo.Connection(memory, memory, transform=0.5, synapse=0.1)\n",
    "    \n",
    "    net.config[mem_conn].trainable = True\n",
    "\n",
    "    net.config[nengo.Probe].synapse = nengo.Lowpass(0.01)\n",
    "    role_probe = nengo.Probe(role_inp) \n",
    "    fill_probe = nengo.Probe(fill_inp)\n",
    "    cue_probe = nengo.Probe(cue_inp)\n",
    "    conv_probe = nengo.Probe(cconv.output)\n",
    "    memory_probe = nengo.Probe(memory)\n",
    "    output_probe = nengo.Probe(ccorr.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model and see how things look with the added binding and unbinding steps playing a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "output_vocab = vocab.create_subset([k for l in range(3) for k in vocab.keys \n",
    "                                    if str(l) in k])\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.subplot(7, 1, 1)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[role_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Role Input\")\n",
    "\n",
    "plt.subplot(7, 1, 2)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[fill_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Filler Input\")\n",
    "\n",
    "plt.subplot(7, 1, 3)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[conv_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Binding\")\n",
    "\n",
    "plt.subplot(7, 1, 4)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Memory\")\n",
    "\n",
    "out = pairs[0,:,:].reshape(t_int*2, dims)\n",
    "plt.subplot(7, 1, 5)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(out, output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Target Memory\")\n",
    "\n",
    "plt.subplot(7, 1, 6)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[cue_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Cue\")\n",
    "\n",
    "plt.subplot(7, 1, 7)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[output_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Retrieval\")\n",
    "plt.xlabel(\"time [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the new model\n",
    "\n",
    "Because the memory is initialized poorly, the model is not able produce the target trajectory very well at all. And while the input cue produces an output vector that is most similar to appropriate semantic pointer (i.e., the filler associated with this input cue), the overall similarity between the network's output and the desired semantic pointer (i.e, `FILL0`) is very small - around 0.15. We can fix this by optimizing the model parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=50, seed=seed)\n",
    "\n",
    "if do_training:\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.00001, momentum=0.8, use_nesterov=True)\n",
    "\n",
    "    train_roles, train_fills, train_pairs, _ = get_binding_data(2500, dims, seed, t_int)\n",
    "    input_feed = {role_inp: train_roles, fill_inp: train_fills}\n",
    "    output_feed = {memory_probe: train_pairs}\n",
    "\n",
    "    print('Training loss before: ', sim.loss(input_feed, output_feed, 'mse'))\n",
    "    sim.train(input_feed, output_feed, optimizer, n_epochs=5, objective='mse')\n",
    "    print('Training loss after: ', sim.loss(input_feed, output_feed, 'mse'))\n",
    "    \n",
    "    sim.save_params('./mem_binding_params')\n",
    "\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4UWFpRjFPd3B0cE0\",\n",
    "        \"mem_binding_params.zip\")\n",
    "    with zipfile.ZipFile(\"mem_binding_params.zip\") as f:\n",
    "        f.extractall()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare results\n",
    "\n",
    "We can visualize the change that results from training in this more complicated model's behavior by plotting the similarities between the states of various network components and the vocabulary of known semantic pointers over time, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.load_params('./mem_binding_params')\n",
    "    sim.run(2 * p_time)\n",
    "\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.subplot(7, 1, 1)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[role_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Role Input\")\n",
    "\n",
    "plt.subplot(7, 1, 2)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[fill_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Filler Input\")\n",
    "\n",
    "plt.subplot(7, 1, 3)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[conv_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Binding\")\n",
    "\n",
    "plt.subplot(7, 1, 4)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[memory_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Memory\")\n",
    "\n",
    "out = pairs[0,:,:].reshape(t_int*2, dims)\n",
    "plt.subplot(7, 1, 5)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(out, output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Target Memory\")\n",
    "\n",
    "plt.subplot(7, 1, 6)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[cue_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Cue\")\n",
    "\n",
    "plt.subplot(7, 1, 7)\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[output_probe], output_vocab))\n",
    "plt.legend(output_vocab.keys, fontsize='x-small', loc='right')\n",
    "plt.ylabel(\"Retrieval\")\n",
    "plt.xlabel(\"time [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the network's memory representation is still not ideal, but it is closer to the target trajectory than before. And as as result, a more accurate retrieval of the semantic pointer associated with the input cue is produced as the network output. It is likely possible to further improve these results by training on larger amount of data for a longer period of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
